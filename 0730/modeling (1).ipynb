{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "modeling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTtmARJLyIyO",
        "outputId": "60845daf-8482-4108-f6c0-47d5cd29d8d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "KkSHBnQ2zhH0"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data 갖고오기"
      ],
      "metadata": {
        "id": "q5JZBnQL0b39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(dir_path=\"/content/drive/MyDrive/SOOKTAT/딥러닝/프로젝트/train\"):\n",
        "    labels = ['Closed', 'Open']\n",
        "    IMG_SIZE = 145\n",
        "    data = []\n",
        "    for label in labels:\n",
        "        path = os.path.join(dir_path, label)\n",
        "        class_num = labels.index(label)\n",
        "        print(class_num)\n",
        "        for img in os.listdir(path): # closed 폴더 안에 _1.jpg, open 폴더 안에 1.jpg \n",
        "            try:\n",
        "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR) #/content/drive/MyDrive/2기 딥러닝 프로젝트/train/Closed/_0.jpg\n",
        "                resized_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\n",
        "                data.append([resized_array, class_num])\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "    return data"
      ],
      "metadata": {
        "id": "8wGc6rvfz2xX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "\n",
        "data_train = get_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-cFUPJkS0C-n",
        "outputId": "7670e4bc-a5d6-4179-e8e6-76a876c97da6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "for feature, label in data_train:\n",
        "    X.append(feature)\n",
        "    y.append(label)"
      ],
      "metadata": {
        "id": "kRLUS7AQ0P18"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "bOpRAaZe0Tnp"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "label_bin = LabelBinarizer()\n",
        "y = label_bin.fit_transform(y)"
      ],
      "metadata": {
        "id": "d-DWJoKb0TzN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "seed = 42\n",
        "test_size = 0.30\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=seed, test_size=test_size)"
      ],
      "metadata": {
        "id": "_1lmgbGH0XzI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "9t_qxHnx0mMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.transforms import transforms\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "ekNgQRR50Z_5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "])"
      ],
      "metadata": {
        "id": "FywmiVWl0qGw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "\n",
        "class eyes_dataset(Dataset):\n",
        "    def __init__(self, x_file_paths, y_file_path, transform=None):\n",
        "        self.x_files = x_file_paths\n",
        "        self.y_files = y_file_path\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = self.x_files[idx]\n",
        "        x = torch.from_numpy(x).float()\n",
        "\n",
        "        y = self.y_files[idx]\n",
        "        y = torch.from_numpy(y).float()\n",
        "\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x_files)"
      ],
      "metadata": {
        "id": "hgduo0VS43R_"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = eyes_dataset(X_train, y_train, transform=train_transform)"
      ],
      "metadata": {
        "id": "9ICyapaL46EM"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "/ model"
      ],
      "metadata": {
        "id": "o5MIXkdO5bBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchsummary import summary\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
        "        self.fc1 = nn.Linear(1536, 512)\n",
        "        self.fc2 = nn.Linear(512, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)),2)\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        x = F.max_pool2d(F.relu(self.conv3(x)), 2)\n",
        "        x = x.reshape(-1, 1536)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "KMzrROw75cSZ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ud0CbWC88-n-",
        "outputId": "ebddb017-6ef6-4f0d-a2e5-b7cb499934ef"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net()\n",
        "model.to('cuda')\n",
        "\n",
        "#os.environ[\"CUDA_VISIBLE_DEVICES\"]\n",
        "\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "epochs = 50"
      ],
      "metadata": {
        "id": "fMoCLpBR5uIo"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    running_acc = 0.0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        input_1, labels = data[0].to('cuda'), data[1].to('cuda')\n",
        "\n",
        "        input = input_1.transpose(1, 3).transpose(2, 3)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        running_acc += accuracy(outputs, labels)\n",
        "\n",
        "        if i % 80 == 79:\n",
        "            print('epoch: [%d/%d] train_loss: %.5f train_acc: %.5f' % (\n",
        "                epoch + 1, epochs, running_loss / 80, running_acc / 80))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print(\"learning finish\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 457
        },
        "id": "BviscRU454C3",
        "outputId": "ca602669-fe81-4425-f6bf-1dd5b127f457"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:560: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-7895099ccc72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    715\u001b[0m                                                   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    716\u001b[0m                                                   \u001b[0mpos_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 717\u001b[0;31m                                                   reduction=self.reduction)\n\u001b[0m\u001b[1;32m    718\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbinary_cross_entropy_with_logits\u001b[0;34m(input, target, weight, size_average, reduce, reduction, pos_weight)\u001b[0m\n\u001b[1;32m   3146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3147\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3148\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Target size ({}) must be the same as input size ({})\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbinary_cross_entropy_with_logits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction_enum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Target size (torch.Size([32, 1])) must be the same as input size (torch.Size([864, 1]))"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(y_pred, y_test):\n",
        "    y_pred_tag = torch.round(torch.sigmoid(y_pred))\n",
        "\n",
        "    correct_results_sum = (y_pred_tag == y_test).sum().float()\n",
        "    acc = correct_results_sum / y_test.shape[0]\n",
        "    acc = torch.round(acc * 100)\n",
        "\n",
        "    return acc"
      ],
      "metadata": {
        "id": "VdaSZ0vb92tB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test"
      ],
      "metadata": {
        "id": "xtbE37gq-HKc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "test_dataset = eyes_dataset(x_test, y_test, transform=test_transform)\n",
        "\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=4)\n",
        "\n",
        "model = Net()\n",
        "model.to('cuda')\n",
        "model.load_state_dict(torch.load(PATH))\n",
        "model.eval()\n",
        "\n",
        "count = 0"
      ],
      "metadata": {
        "id": "shF4cQ5U99c2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    total_acc = 0.0\n",
        "    acc = 0.0\n",
        "    for i, test_data in enumerate(test_dataloader, 0):\n",
        "        data, labels = test_data[0].to('cuda'), test_data[1].to('cuda')\n",
        "\n",
        "        data = data.transpose(1, 3).transpose(2, 3)\n",
        "\n",
        "        outputs = model(data)\n",
        "\n",
        "        acc = accuracy(outputs, labels)\n",
        "        total_acc += acc\n",
        "\n",
        "        count = i\n",
        "\n",
        "    print('avarage acc: %.5f' % (total_acc/count),'%')\n",
        "\n",
        "print('test finish!')"
      ],
      "metadata": {
        "id": "QoXHtmQf-BWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Bvjzk3hW-DUe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}